# **‚úÖ T·ªïng quan n·ªôi dung trong b√†i b√°o:**

## **1. Datasets**

- **JSRT Dataset** (247 ·∫£nh):

  - 100 ·∫£nh ung th∆∞ (malignant)
  - 54 ·∫£nh l√†nh t√≠nh (benign)
  - 93 ·∫£nh kh√¥ng c√≥ nodule

- **ChestX-ray14 Dataset** (\~112,120 ·∫£nh):

  - C√≥ 14 b·ªánh, bao g·ªìm "Nodule"
  - D√πng ƒë·ªÉ **hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n bi·ªát nodule / non-nodule**

---

## **2. C√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω ·∫£nh**

- TƒÉng t∆∞∆°ng ph·∫£n: Histogram Equalization
- L·ªçc nhi·ªÖu: Median Filter (3x3)
- Resize: 224x224
- Chu·∫©n h√≥a: theo mean/std c·ªßa ImageNet

---

## **3. M√¥ h√¨nh v√† chi·∫øn l∆∞·ª£c Transfer Learning**

#### üî∑ Base Model

- DenseNet-121 (pretrained tr√™n ImageNet)
- Layer cu·ªëi thay b·∫±ng 1 node sigmoid

#### üî∑ **Model A**

- Hu·∫•n luy·ªán t·ª´ Base Model tr√™n **ChestX-ray14**
- Ph√¢n bi·ªát **nodule vs non-nodule**
- D·ªØ li·ªáu l·ªõn, h·ªçc t·ªët, ƒë∆∞·ª£c coi l√† ‚Äúchuy√™n gia ph√¢n bi·ªát nodule‚Äù

#### üî∑ **Model B**

- Hu·∫•n luy·ªán Base Model tr√™n **JSRT**
- Ph√¢n bi·ªát **lung cancer vs non-cancer** (malignant vs benign + no nodule)
- 10-fold cross-validation
- TƒÉng c∆∞·ªùng d·ªØ li·ªáu: xoay ¬±30¬∞, l·∫≠t ngang

#### üî∑ **Model C**

- Hu·∫•n luy·ªán l·∫°i t·ª´ **Model A** tr√™n **JSRT**
- T·ª©c l√† **transfer 2 l·∫ßn**: ImageNet ‚Üí ChestX-ray14 ‚Üí JSRT
- M·ª•c ti√™u: ph√¢n bi·ªát malignant / non-malignant
- K·∫øt qu·∫£ t·ªët nh·∫•t trong b√†i b√°o

---

## **4 . ƒê√°nh gi√°**

| Model | Dataset      | Nhi·ªám v·ª•                           | Accuracy        | Specificity  | Sensitivity  |
| ----- | ------------ | ---------------------------------- | --------------- | ------------ | ------------ |
| A     | ChestX-ray14 | Nodule vs Non-nodule               | **84.02%**      | 85.34%       | 82.71%       |
| B     | JSRT         | Cancer vs Non-cancer               | 65.51¬±7.67%     | 80.95¬±20.59% | 45.67¬±21.36% |
| C     | ChestX+JSRT  | Cancer vs Non-cancer (fine-tune A) | **74.43¬±6.01%** | 74.96¬±9.85%  | 74.68¬±15.33% |

# **C√°c b∆∞·ªõc th·ª±c hi·ªán chi ti·∫øt theo b√†i b√°o (vi·∫øt l·∫°i theo m√¥i tr∆∞·ªùng Google Colab, s·ª≠ d·ª•ng images\_001 duy nh·∫•t)**

## Ph·∫ßn 1: Chu·∫©n b·ªã d·ªØ li·ªáu ‚Äì ChestX-ray14 (Google Colab, ch·ªâ d√πng images\_001)

### B∆∞·ªõc 1.1: T·∫£i d·ªØ li·ªáu

- Upload file `Data_Entry_2017.csv` v√† th∆∞ m·ª•c `images_001` l√™n Google Drive.
- Mount Google Drive trong Colab:

```python
from google.colab import drive 
drive.mount('/content/drive')
```

### B∆∞·ªõc 1.2: G√°n nh√£n v√† ƒë·ªçc th√¥ng tin ·∫£nh

```python
import pandas as pd

csv_path = '/content/drive/MyDrive/ChestXray/Data_Entry_2017.csv'
df = pd.read_csv(csv_path)

# G√°n nh√£n binary: 1 n·∫øu c√≥ Nodule, 0 n·∫øu kh√¥ng
df['Label'] = df['Finding Labels'].apply(lambda x: 1 if 'Nodule' in x else 0)
print('S·ªë ·∫£nh Positive (label=1):', df[df['Label'] == 1].shape[0])
print('S·ªë ·∫£nh Negative (label=0):', df[df['Label'] == 0].shape[0])
```

### B∆∞·ªõc 1.3: Chia train/val/test theo t·ª∑ l·ªá b√†i b√°o (tr√™n t·∫≠p ƒë√£ l·ªçc s·∫µn)

```python
from sklearn.utils import shuffle

# T√°ch positive v√† negative
df_positive = df[df['Label'] == 1]
df_negative = df[df['Label'] == 0]

# L·∫•y m·∫´u nh·ªè ƒë·ªÉ demo tr√™n images_001 (ch·ªânh s·ªë l∆∞·ª£ng tu·ª≥ theo th·ª±c t·∫ø ·∫£nh)
pos_train = df_positive.sample(n=200, random_state=42)
pos_val = df_positive.drop(pos_train.index).sample(n=50, random_state=42)
pos_test = df_positive.drop(pos_train.index).drop(pos_val.index).sample(n=30, random_state=42)

neg_train = df_negative.sample(n=3000, random_state=42)
neg_val = df_negative.drop(neg_train.index).sample(n=500, random_state=42)
neg_test = df_negative.drop(neg_train.index).drop(neg_val.index).sample(n=500, random_state=42)

train_df = pd.concat([pos_train, neg_train]).sample(frac=1).reset_index(drop=True)
val_df = pd.concat([pos_val, neg_val]).sample(frac=1).reset_index(drop=True)
test_df = pd.concat([pos_test, neg_test]).sample(frac=1).reset_index(drop=True)

print('Train:', train_df.shape)
print('Validation:', val_df.shape)
print('Test:', test_df.shape)
```

## Ph·∫ßn 2: Ti·ªÅn x·ª≠ l√Ω ·∫£nh (Google Colab, images\_001)

```python
import cv2
import numpy as np
from tqdm import tqdm
import os

# Th∆∞ m·ª•c ·∫£nh g·ªëc v√† th∆∞ m·ª•c l∆∞u file .npy
image_folder = '/content/drive/MyDrive/ƒê·ªì √°n chuy√™n ng√†nh Tr√≠ tu·ªá nh√¢n t·∫°o/Dataset/ChestX-ray14/images'
output_folder = '/content/drive/MyDrive/ƒê·ªì √°n chuy√™n ng√†nh Tr√≠ tu·ªá nh√¢n t·∫°o/Dataset/ChestX-ray14/processed_npy'
os.makedirs(output_folder, exist_ok=True)

# L·ªçc l·∫°i c√°c dataframe ch·ªâ gi·ªØ ·∫£nh c√≥ th·∫≠t
available_images = set(os.listdir(image_folder))
for df_ in [train_df, val_df, test_df]:
    df_.drop(df_[~df_['Image Index'].isin(available_images)].index, inplace=True)
    df_.reset_index(drop=True, inplace=True)

# Chu·∫©n h√≥a theo ImageNet
imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std = [0.229, 0.224, 0.225]

def preprocess_image(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f"[L·ªñI] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ·∫£nh: {image_path}")
        return None
    img = cv2.equalizeHist(img)
    img = cv2.medianBlur(img, 3)
    img = cv2.resize(img, (224, 224))
    img = img / 255.0
    img = np.stack([img, img, img], axis=-1)
    for i in range(3):
        img[:, :, i] = (img[:, :, i] - imagenet_mean[i]) / imagenet_std[i]
    return img.astype(np.float32)

# Ti·ªÅn x·ª≠ l√Ω c·∫£ 3 t·∫≠p train/val/test
for subset_df, name in zip([train_df, val_df, test_df], ['train', 'val', 'test']):
    for idx, row in tqdm(subset_df.iterrows(), total=len(subset_df), desc=f'Processing {name}'):
        image_name = row['Image Index']
        image_path = os.path.join(image_folder, image_name)
        processed_img = preprocess_image(image_path)
        if processed_img is not None:
            save_name = os.path.splitext(image_name)[0] + '.npy'
            save_path = os.path.join(output_folder, save_name)
            np.save(save_path, processed_img)

print("‚úÖ ƒê√£ ti·ªÅn x·ª≠ l√Ω to√†n b·ªô ·∫£nh v√† l∆∞u d∆∞·ªõi d·∫°ng .npy")
```

## Ph·∫ßn 3: X√¢y d·ª±ng v√† hu·∫•n luy·ªán Model A (Google Colab)

### B∆∞·ªõc 3.1: T√≠nh class weights ƒë·ªÉ x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng

```python
from sklearn.utils import class_weight
import numpy as np

class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.array([0, 1]),
    y=train_df['Label'].values
)

class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
print("Class weights:", class_weight_dict)
```

### B∆∞·ªõc 3.2: Custom Generator ƒë·ªçc ·∫£nh .npy

```python
import tensorflow as tf
import os

class NPYDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, dataframe, batch_size, data_dir, shuffle=True, **kwargs):
        super().__init__(**kwargs)
        super().__init__()
        self.df = dataframe.reset_index(drop=True)
        self.batch_size = batch_size
        self.data_dir = data_dir
        self.shuffle = shuffle
        self.indices = np.arange(len(self.df))
        self.on_epoch_end()

    def __len__(self):
        return int(np.ceil(len(self.df) / self.batch_size))

    def __getitem__(self, index):
        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]
        batch_images = []
        batch_labels = []

        for i in batch_indices:
            row = self.df.iloc[i]
            image_name = os.path.splitext(row['Image Index'])[0] + '.npy'
            image_path = os.path.join(self.data_dir, image_name)

            if not os.path.exists(image_path):
                continue

            image = np.load(image_path)
            label = row['Label']
            batch_images.append(image)
            batch_labels.append(label)

        return np.array(batch_images), np.array(batch_labels)

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)
```

### B∆∞·ªõc 3.3: T·∫°o train/val generator

```python
train_generator = NPYDataGenerator(
    dataframe=train_df,
    batch_size=32,
    data_dir='/content/drive/MyDrive/ƒê·ªì √°n chuy√™n ng√†nh Tr√≠ tu·ªá nh√¢n t·∫°o/Dataset/ChestX-ray14/processed_npy/'
)

val_generator = NPYDataGenerator(
    dataframe=val_df,
    batch_size=32,
    data_dir='/content/drive/MyDrive/ƒê·ªì √°n chuy√™n ng√†nh Tr√≠ tu·ªá nh√¢n t·∫°o/Dataset/ChestX-ray14/processed_npy/',
    shuffle=False
)
```

### B∆∞·ªõc 3.4: X√¢y d·ª±ng m√¥ h√¨nh DenseNet121

```python
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras import layers, models, optimizers

base_model = DenseNet121(include_top=False, input_shape=(224, 224, 3), pooling='avg', weights='imagenet')
x = layers.Dense(1, activation='sigmoid')(base_model.output)
model = models.Model(inputs=base_model.input, outputs=x)

model.compile(
    optimizer=optimizers.Adam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy']
)
```

### B∆∞·ªõc 3.5: Hu·∫•n luy·ªán m√¥ h√¨nh

```python
model.fit(
    train_generator,
    epochs=10,
    validation_data=val_generator,
    class_weight=class_weight_dict
)
```

### B∆∞·ªõc 3.6: L∆∞u m√¥ h√¨nh A sau khi hu·∫•n luy·ªán

```
model_path = '/content/drive/MyDrive/ƒê·ªì √°n chuy√™n ng√†nh Tr√≠ tu·ªá nh√¢n t·∫°o/models/model_A_chestxray14.h5'

model.save(model_path)

print("‚úÖ ƒê√£ l∆∞u Model A t·∫°i:", model_path)
```

## Ph·∫ßn 4: ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p ki·ªÉm tra (test set)

### B∆∞·ªõc 4.1: ƒê√°nh gi√° ch·ªâ s·ªë loss v√† accuracy

```python
# T·∫°o generator cho t·∫≠p test
test_generator = NPYDataGenerator(
    dataframe=test_df,
    batch_size=32,
    data_dir='/content/drive/MyDrive/ƒê·ªì √°n chuy√™n ng√†nh Tr√≠ tu·ªá nh√¢n t·∫°o/Dataset/ChestX-ray14/processed_npy/',
    shuffle=False
)

# ƒê√°nh gi√° m√¥ h√¨nh
loss, accuracy = model.evaluate(test_generator)
print("‚úÖ Test accuracy:", accuracy)
print("‚úÖ Test loss:", loss)
```

### B∆∞·ªõc 4.2: V·∫Ω bi·ªÉu ƒë·ªì Accuracy v√† Loss theo t·ª´ng Epoch

```python
import matplotlib.pyplot as plt

# Accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy theo t·ª´ng Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss theo t·ª´ng Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

## Ph·∫ßn 5: D·ª± ƒëo√°n v√† tr·ª±c quan h√≥a k·∫øt qu·∫£

### B∆∞·ªõc 5.1: D·ª± ƒëo√°n v√† v·∫Ω Confusion Matrix

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# D·ª± ƒëo√°n nh√£n t·ª´ m√¥ h√¨nh
y_true = test_df['Label'].values
predictions = model.predict(test_generator)
y_pred = (predictions > 0.5).astype(int)

# T·∫°o v√† hi·ªÉn th·ªã confusion matrix
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()
```

### B∆∞·ªõc 5.2: Hi·ªÉn th·ªã ·∫£nh d·ª± ƒëo√°n sai (3x3 l∆∞·ªõi)

```python
import random

wrong_indices = np.where(y_true != y_pred.reshape(-1))[0]
sample_wrong = random.sample(list(wrong_indices), min(9, len(wrong_indices)))

fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(16, 12))
axes = axes.flatten()

for idx, i in enumerate(sample_wrong):
    img_name = test_df.iloc[i]['Image Index']
    npy_path = os.path.join('/content/drive/MyDrive/ƒê·ªì √°n chuy√™n ng√†nh Tr√≠ tu·ªá nh√¢n t·∫°o/Dataset/ChestX-ray14/processed_npy/', img_name.replace('.png', '.npy'))
    img = np.load(npy_path)
    axes[idx].imshow(img[:, :, 0], cmap='gray')
    axes[idx].set_title(f"Pred: {y_pred[i][0]}, True: {y_true[i]}")
    axes[idx].axis('off')

plt.tight_layout()
plt.show()
```

## Ph·∫ßn 6: Chu·∫©n b·ªã d·ªØ li·ªáu JSRT

### B∆∞·ªõc 6.1: G√°n nh√£n t·ª´ jsrt\_metadata.csv

````python
import pandas as pd

# ƒê·ªçc file metadata c·ªßa JSRT
label_path = '/content/drive/MyDrive/JSRT/jsrt_metadata.csv'
df_jsrt = pd.read_csv(label_path)

# G·∫Øn t√™n file ·∫£nh t∆∞∆°ng ·ª©ng (.png)
df_jsrt['Image Index'] = df_jsrt['study_id'].apply(lambda x: x + '.png')

# G√°n nh√£n: 1 n·∫øu malignant, ng∆∞·ª£c l·∫°i l√† 0 (c·∫©n th·∫≠n v·ªõi NaN)
df_jsrt['Label'] = df_jsrt['diagnosis'].apply(lambda x: 1 if isinstance(x, str) and x.lower() == 'malignant' else 0)

# In th·ªëng k√™
print('‚úÖ S·ªë ·∫£nh ung th∆∞ (label=1):', df_jsrt[df_jsrt['Label'] == 1].shape[0])
print('‚úÖ S·ªë ·∫£nh kh√¥ng ung th∆∞ (label=0):', df_jsrt[df_jsrt['Label'] == 0].shape[0])
```python
import pandas as pd

# ƒê·ªçc file metadata c·ªßa JSRT
label_path = '/content/drive/MyDrive/JSRT/jsrt_metadata.csv'
df_jsrt = pd.read_csv(label_path)

# G·∫Øn t√™n file ·∫£nh t∆∞∆°ng ·ª©ng (.png)
df_jsrt['Image Index'] = df_jsrt['study_id'].apply(lambda x: x + '.png')

# G√°n nh√£n: 1 n·∫øu malignant, ng∆∞·ª£c l·∫°i l√† 0
df_jsrt['Label'] = df_jsrt['diagnosis'].apply(lambda x: 1 if x.lower() == 'malignant' else 0)

# In th·ªëng k√™
print('‚úÖ S·ªë ·∫£nh ung th∆∞ (label=1):', df_jsrt[df_jsrt['Label'] == 1].shape[0])
print('‚úÖ S·ªë ·∫£nh kh√¥ng ung th∆∞ (label=0):', df_jsrt[df_jsrt['Label'] == 0].shape[0])
````

### B∆∞·ªõc 6.2: Ti·ªÅn x·ª≠ l√Ω ·∫£nh (gi·ªëng nh∆∞ ChestX-ray14)

```python
import os
import cv2
import numpy as np
from tqdm import tqdm

input_folder = '/content/drive/MyDrive/JSRT/images'
output_folder = '/content/drive/MyDrive/JSRT/processed_npy'
os.makedirs(output_folder, exist_ok=True)

imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std = [0.229, 0.224, 0.225]

def preprocess_image(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f"[L·ªñI] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ·∫£nh: {image_path}")
        return None
    img = cv2.equalizeHist(img)
    img = cv2.medianBlur(img, 3)
    img = cv2.resize(img, (224, 224))
    img = img / 255.0
    img = np.stack([img, img, img], axis=-1)
    for i in range(3):
        img[:, :, i] = (img[:, :, i] - imagenet_mean[i]) / imagenet_std[i]
    return img.astype(np.float32)

# Ti·ªÅn x·ª≠ l√Ω to√†n b·ªô JSRT
for idx, row in tqdm(df_jsrt.iterrows(), total=len(df_jsrt), desc='Processing JSRT'):
    img_name = row['Image Index']
    img_path = os.path.join(input_folder, img_name)
    processed = preprocess_image(img_path)
    if processed is not None:
        save_name = img_name.replace('.png', '.npy')
        save_path = os.path.join(output_folder, save_name)
        np.save(save_path, processed)

print("‚úÖ ƒê√£ l∆∞u to√†n b·ªô ·∫£nh JSRT d∆∞·ªõi d·∫°ng .npy")
```

### B∆∞·ªõc 6.3: T√≠nh class weights ƒë·ªÉ x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng

```python
from sklearn.utils import class_weight
import numpy as np

class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.array([0, 1]),
    y=train_df['Label'].values
)

class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
print("Class weights:", class_weight_dict)
```

## Ph·∫ßn 7: Hu·∫•n luy·ªán Model B ‚Äì t·ª´ ƒë·∫ßu v·ªõi JSRT



### B∆∞·ªõc 7.1: Kh·ªüi t·∫°o DenseNet121 t·ª´ ImageNet

\- Kh√¥ng d√πng Model A

\- Add t·∫ßng ƒë·∫ßu ra: \`Dense(1, activation='sigmoid')\`



### B∆∞·ªõc 7.2: √Åp d·ª•ng 10-fold Cross Validation

\- M·ªói fold hu·∫•n luy·ªán ri√™ng

\- T·ªïng h·ª£p k·∫øt qu·∫£: accuracy, sensitivity, specificity



### B∆∞·ªõc 7.3: Data Augmentation

\- L·∫≠t ·∫£nh

\- Xoay ¬±30¬∞



### B∆∞·ªõc 7.4: ƒê√°nh gi√° m√¥ h√¨nh

\- Confusion Matrix

\- Hi·ªÉn th·ªã ·∫£nh d·ª± ƒëo√°n sai





## Ph·∫ßn 8: Hu·∫•n luy·ªán Model C ‚Äì fine-tune t·ª´ Model A



### B∆∞·ªõc 8.1: Load Model A ƒë√£ hu·∫•n luy·ªán (ChestX-ray14)

\- \`model\_A\_chestxray14.h5\`



### B∆∞·ªõc 8.2: Fine-tune v·ªõi JSRT

\- C√≥ th·ªÉ freeze m·ªôt ph·∫ßn ho·∫∑c to√†n b·ªô base

\- Thay layer ƒë·∫ßu ra n·∫øu c·∫ßn



### B∆∞·ªõc 8.3: Hu·∫•n luy·ªán + Augmentation

\- C√πng k·ªπ thu·∫≠t nh∆∞ Model B

\- C√≥ th·ªÉ d√πng l·∫°i folds ƒë√£ chia ·ªü Ph·∫ßn 6



### B∆∞·ªõc 8.4: ƒê√°nh gi√° v√† so s√°nh v·ªõi Model B

\- Confusion Matrix

\- Bi·ªÉu ƒë·ªì

\- So s√°nh k·∫øt qu·∫£ gi·ªØa Model B v√† Model C





